# Memory: 2026-02-16

## VLA vs SFT Decision

**Realized VLA was redundant** with existing SFT pipeline - both predict trajectories from visual features.

**Decision:** Extend existing SFT instead of creating isolated VLA.

## Language Conditioning Extension

Created `training/sft/language_conditioning.py`:

- Abstract `LanguageEncoderBase` class
- Factory: `create_language_encoder(config)`
- Supported types: `bert`, `llama`, `minimax`, `gpt`, `simple`
- Default: BERT (fast fallback)

**Key insight:** BERT is from 2018 - old. MiniMax-2.5 is 2025/2026 - most capable.

**Usage:**
```python
config = LanguageConditioningConfig(language_encoder_type="minimax")
model = LanguageConditionedSFT(config)
```

## Architecture

```
Visual (SSL) ──┐
                ↓
Language ──→ [Fusion] → Waypoint Head → Trajectory
```

Keeps CoT + multi-trajectory from existing SFT, adds language conditioning.
