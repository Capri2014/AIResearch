# Daily Notes - 2026-02-15

## Pipeline PR #6 - RL Evaluation Metrics Hardening

### Completed

**Commit:** `960446b` - Add deterministic evaluation for SFT vs RL policy comparison

**Changes:**
- `training/rl/toy_waypoint_env.py`: Added `policy_sft` and `policy_rl_refined` heuristics with lookahead and predictive speed control. Added `seed` parameter for reproducibility.
- `training/rl/compare_sft_vs_rl.py`: New script for deterministic policy comparison. Runs both policies on identical seeds, outputs `metrics.json` files, prints 3-line summary report.

**Output Example:**
```
ADE: 20.79m (SFT) → 21.02m (RL) [-1%]
FDE: 45.72m (SFT) → 45.69m (RL) [+0%]
Success: 0% (SFT) → 0% (RL) [+0%]
```

**Metrics Output:**
- `out/eval/<run_id>_sft/metrics.json`
- `out/eval/<run_id>_rl/metrics.json`

### Next Steps

- Run more extensive evaluation (100+ episodes) to get statistically significant results
- Integrate with actual PPO-trained delta head from `run_ppo_toy_delta.py`
- Compare learned delta policies against SFT baseline

### Notes

- PR pushed to `feature/roadmap-update-todos` branch
- Both policies currently show ~0% success rate on toy environment (needs tuning)
- Evaluation infrastructure is now in place for proper RL policy comparison
