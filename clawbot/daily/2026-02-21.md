# Daily Notes - 2026-02-21

## Pipeline PR #5: PPO Residual Delta-Waypoint Training

### Summary
Added PPO stub for RL refinement after SFT with toy waypoint environment. Implements residual delta-waypoint learning where final_waypoints = SFT_waypoints + delta_head(encoding).

### Commits
- `feature/daily-2026-02-21-e` - feat(rl): Add PPO residual delta-waypoint training with toy environment

### New Files
- `training/rl/waypoint_env.py` - Toy kinematics environment for waypoint testing
- `training/rl/ppo_residual_waypoint.py` - PPO with residual delta learning
- `training/rl/__init__.py` - Module initialization

### Architecture
```
final_waypoints = sft_waypoints + delta_head(z)
```

### Key Components
- **WaypointEnv**: Simple 2D navigation with state (x, y, vx, vy, goal_x, goal_y)
- **SFTWaypointModel**: Mock SFT model (frozen) for baseline waypoint predictions
- **DeltaWaypointHead**: Trainable residual head
- **PPOResidualWaypointAgent**: PPO training with GAE, value loss, entropy bonus
- **Metrics**: episode_rewards, goals_reached, policy/value losses

### Results (50 episodes)
- Final avg reward: -3496.36
- Goal rate: 0.10

### PR
- Branch: `feature/daily-2026-02-21-e`
- URL: https://github.com/Capri2014/AIResearch/pull/new/feature/daily-2026-02-21-e

---

## Pipeline PR #4: RL to CARLA Pipeline Integration

### Summary
Added end-to-end pipeline script that integrates RL training output with CARLA closed-loop evaluation.

### Commits
- `77796a0` - feat(eval): Add RL to CARLA pipeline for end-to-end evaluation

### New Files
- `training/eval/run_rl_to_carla_pipeline.py` - Pipeline integration script

### Architecture
```
RL training output → Best checkpoint selection → CARLA closed-loop eval → Comparison report
```

### Key Features
- **Automatic checkpoint selection**: Selects best checkpoint from training metrics (by reward)
- **SFT comparison**: Supports --compare-sft flag to compare SFT vs RL in CARLA
- **Metrics**: Route completion, collision rate improvements
- **Smoke test**: --smoke flag for testing without CARLA

### Usage
```bash
# Evaluate RL model in CARLA
python -m training.eval.run_rl_to_carla_pipeline \
  --rl-run-dir out/rl_delta_waypoint/2026-02-20_19-31-52 \
  --output-dir out/rl_carla_eval

# Compare SFT vs RL
python -m training.eval.run_rl_to_carla_pipeline \
  --rl-run-dir out/rl_delta_waypoint/2026-02-20_19-31-52 \
  --output-dir out/rl_carla_eval \
  --compare-sft

# Smoke test
python -m training.eval.run_rl_to_carla_pipeline \
  --rl-run-dir out/rl_delta_waypoint/2026-02-20_19-31-52 \
  --output-dir out/rl_carla_eval \
  --smoke
```

### PR
- Branch: `feature/daily-2026-02-21-d`
- URL: https://github.com/Capri2014/AIResearch/pull/new/feature/daily-2026-02-21-d

---

## Pipeline PR #3: Waypoint Trajectory Smoothing

### Summary
Added trajectory smoothing module for post-processing waypoint predictions to ensure kinematic feasibility.

### Commits
- `9be2cb5` - feat(rl): Add waypoint trajectory smoothing for kinematic feasibility

### New Files
- `training/rl/waypoint_smoothing.py` - Core smoothing module
- `training/rl/test_waypoint_smoothing.py` - Smoke tests

### Architecture
```
final_waypoints = smooth(delta_head(z) + sft_waypoints)
```

### Key Features
- **Smoothing methods**: Exponential, Moving Average, Savitzky-Golay
- **Kinematic feasibility**: Speed/acceleration limits enforcement
- **Learnable smoothing**: PyTorch module for differentiable training
- **ADE improvement**: 0.631m → 0.477m on noisy trajectories

### Usage
```python
from training.rl.waypoint_smoothing import (
    WaypointSmootherConfig,
    smooth_waypoints
)

config = WaypointSmootherConfig(
    smoothing_window=5,
    apply_speed_profile=True,
    max_speed=15.0,
    max_acceleration=3.0
)
smoothed = smooth_waypoints(waypoints, config, method="savgol")
```

### PR
- Branch: `feature/daily-2026-02-21-c`
- URL: https://github.com/Capri2014/AIResearch/pull/new/feature/daily-2026-02-21-c

---

## Pipeline PR #2: GRPO Delta-Waypoint Training

### Summary
Added GRPO (Group Relative Policy Optimization) training for residual delta-waypoint learning after SFT.

### Commits
- `c95df22` - feat(rl): Add GRPO delta-waypoint training for RL refinement after SFT

### New Files
- `training/rl/train_grpo_delta_waypoint.py` - GRPO training script
- `training/rl/test_grpo_delta_smoke.py` - Smoke tests
- `training/rl/sft_checkpoint_loader.py` - SFT checkpoint loading

### Architecture
```
final_waypoints = sft_waypoints + delta_head(z)
```

### Key Features
- Frozen SFT model as base predictor
- Trainable delta head with GRPO optimization
- Group-relative advantage estimation
- Integration with SFT checkpoint loader

### GRPO vs PPO
- GRPO doesn't need a value function
- Better scaling to large models
- Simpler, more stable training

### Usage
```bash
python -m training.rl.train_grpo_delta_waypoint \
  --sft-checkpoint out/waypoint_bc/run_001/model.pt \
  --output-dir out/grpo_delta \
  --num-episodes 1000
```

### PR
- Branch: `feature/daily-2026-02-21-b`
- URL: https://github.com/Capri2014/AIResearch/pull/new/feature/daily-2026-02-21-b
