# 2026-02-18 Daily Notes

## Pipeline PR #1 (Daily Cadence)

**Focus:** RL Checkpoint Selection with Policy Entropy

### Changes

- **Updated:** `training/rl/train_rl_delta_waypoint.py` - Checkpoint selection with policy entropy metrics

### Key Additions

1. **Policy Entropy Tracking**
   - Added `policy_entropy` field to evaluation metrics
   - Tracks entropy per episode for monitoring policy exploration
   - Stored in `entropy_history.json` with episode-wise records

2. **Best Checkpoint Selection**
   - Added `_save_best_checkpoint()` method for entropy-based checkpointing
   - Higher entropy = more exploration = better for RL
   - Saves `best_entropy.pt` when new best entropy is found
   - Includes metadata: episode, entropy, config

3. **Entropy History Tracking**
   - Added `entropy_history` list and `_save_entropy_history()` method
   - Records entropy at each eval interval
   - Best entropy and episode saved for easy retrieval

4. **Training Summary Enhancement**
   - Added `best_checkpoint` section to `train_metrics.json`
   - Includes path, episode, and entropy value

### Metrics Schema

```python
# New eval_info structure
eval_info = {
    "mean_delta_norm": float,      # Mean delta norm
    "max_delta_norm": float,       # Max delta norm
    "std_delta_norm": float,       # Std delta norm
    "policy_entropy": float,       # NEW: Policy entropy
}

# New entropy_history.json structure
{
    "episodes": [1, 10, 20, ...],
    "entropy": [0.5, 0.6, 0.7, ...],
    "best_entropy": 0.9,
    "best_episode": 150,
}

# New best_checkpoint in train_metrics.json
"best_checkpoint": {
    "path": "out/.../best_entropy.pt",
    "episode": 150,
    "entropy": 0.9,
}
```

### Usage

```bash
# Training automatically tracks entropy and saves best checkpoint
python -m training.rl.train_rl_delta_waypoint \
  --out-dir out/rl_delta_waypoint_v0/run_001 \
  --episodes 500

# After training, best checkpoint is saved at:
# out/rl_delta_waypoint_v0/run_001/best_entropy.pt

# Entropy history for analysis:
# out/rl_delta_waypoint_v0/run_001/entropy_history.json
```

### Why Entropy Matters

- **Higher entropy** = more diverse action distribution = policy explores more
- **Lower entropy** = policy becomes deterministic (may overfit)
- Entropy-based checkpoint selection helps find well-regularized policies
- Complements reward-based selection with exploration quality signal

### Next Steps

- [ ] Run training with new entropy tracking
- [ ] Compare entropy curves across different seeds
- [ ] Add entropy-based early stopping (stop if entropy drops too low)
- [ ] Integrate with CARLA evaluation for closed-loop validation

---

## Pipeline Context

Driving-first pipeline:
```
Waymo episodes → SSL pretrain → waypoint BC (SFT) → RL refinement → eval (ADE/FDE/entropy)
```

Today's contribution:
- RL training now has **best checkpoint selection** based on policy entropy
- Enables automated model selection for deployment
- Provides exploration quality signal alongside reward
