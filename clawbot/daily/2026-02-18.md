# 2026-02-18 Daily Notes

## Pipeline PR #5 (Daily Cadence)

**Focus:** PPO Stub for RL Refinement After SFT

### Changes

- **Added:** `training/rl/ppo_rl_refine_stub.py` - PPO stub implementing residual delta-waypoint learning
  - **SFT initialization**: `SFTWaypointModelStub` for loading pretrained waypoint BC models
  - **Residual delta head**: Learns `Δ = (y - ŷ) / σ` normalized correction
  - **Architecture**: `final_waypoints = sft_waypoints + delta_head(z)`
  - **PPO training**: GAE, clipping, value loss, entropy bonus
  - **Toy environment**: `ToyWaypointEnv` integration for kinematics testing
  - **Artifacts**: `out/<run_id>/metrics.json` and `train_metrics.json`

### Architecture

```
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│   SFT Model     │     │  Delta Head     │     │   Value Head    │
│  (frozen)       │     │  (trainable)    │     │  (trainable)    │
└────────┬────────┘     └────────┬────────┘     └────────┬────────┘
         │                       │                       │
         └───────────────────────┼───────────────────────┘
                                 │
                    ┌────────────▼────────────┐
                    │   final_waypoints =     │
                    │   sft_waypoints + Δ     │
                    └─────────────────────────┘
```

### Key Components

```python
@dataclass
class PPORLConfig:
    """PPO configuration for residual delta-waypoint learning."""
    sft_model_path: Optional[str] = None
    horizon: int = 20
    delta_hidden_dim: int = 64
    delta_lr: float = 1e-4
    ppo_lr: float = 3e-4
    clip_epsilon: float = 0.2
    gamma: float = 0.99
    lam: float = 0.95

class SFTWaypointModelStub(nn.Module):
    """Stub for SFT waypoint prediction (loads pretrained weights)."""
    ...

class DeltaWaypointHead(nn.Module):
    """Residual delta-waypoint head."""
    def forward(self, z) -> Tuple[torch.Tensor, torch.Tensor]:
        delta = self.delta_net(z)        # Δ prediction
        log_sigma = self.uncertainty_net(z)  # uncertainty
        return delta, log_sigma

class WaypointPolicy(nn.Module):
    """Combined SFT + delta policy."""
    def forward(self, z):
        sft_waypoints = self.sft_model(z)
        delta, _ = self.delta_head(z)
        value = self.value_head(z)
        return sft_waypoints, delta, value
```

### Loss Function

```
Loss = NLL + MSE + KL regularization

Δ_norm = (y - ŷ) / σ    # normalized delta
```

### Usage

```bash
# Run PPO training stub
python -m training.rl.ppo_rl_refine_stub --episodes 100 --lr 3e-4

# Output artifacts
out/ppo-delta-<timestamp>/metrics.json
out/ppo-delta-<timestamp>/train_metrics.json
```

### Output Format

```json
{
  "run_id": "ppo-delta-20260218-193000-abc123",
  "total_episodes": 100,
  "avg_reward": 2.34,
  "std_reward": 0.45,
  "avg_ade": 3.21,
  "avg_fde": 7.85,
  "best_reward": 5.12
}
```

### Pipeline Integration

RL refinement stage after SFT:
```
Waymo episodes → SSL pretrain → waypoint BC (SFT) → RL refinement (PPO) → eval
```

### Next Steps

- [ ] Integrate with actual SFT checkpoint loader
- [ ] Add KL regularization between SFT and RL policies
- [ ] CARLA closed-loop evaluation
- [ ] Compare offline ADE/FDE with closed-loop metrics
