# 2026-02-18 Daily Notes

## Pipeline PR #6 (Daily Cadence)

**Focus:** ADE/FDE metrics and comparison loader for RL refinement

### Changes

- **Extended:** `training/rl/eval_toy_waypoint_env.py` - Added ADE/FDE computation
  - ADE (Average Displacement Error): mean distance to all waypoints per episode
  - FDE (Final Displacement Error): distance to final waypoint
  - Summary statistics: ade_mean, ade_std, fde_mean, fde_std, success_rate
  - Compatible with `data/schema/metrics.json` (domain="rl")

- **Added:** `training/rl/compare_metrics.py` - SFT vs RL comparison loader
  - Accepts directories or file paths containing `metrics.json`
  - Prints 3-line summary report with percentage improvements
  - Usage: `python -m training.rl.compare_metrics -b <sft_run> -c <rl_run>`

### Usage

```bash
# Run SFT evaluation
python -m training.rl.eval_toy_waypoint_env --policy sft --episodes 20 --seed-base 42

# Run RL evaluation
python -m training.rl.eval_toy_waypoint_env --policy rl --episodes 20 --seed-base 42

# Compare results
python -m training.rl.compare_metrics -b out/eval/<sft_run_id> -c out/eval/<rl_run_id>
```

### Example Output

```
============================================================
COMPARISON REPORT
============================================================

SFT (20260218-213220):
  ADE: 9.76m
  FDE: 29.91m
  Success: 0%

RL (20260218-213223):
  ADE: 9.12m
  FDE: 28.81m
  Success: 0%

------------------------------------------------------------
3-LINE SUMMARY:
------------------------------------------------------------
ADE: 9.76m (SFT) → 9.12m (RL)
FDE: 29.91m (SFT) → 28.81m (RL)
Success: 0% (SFT) → 0% (RL)
============================================================
```

### Next Steps

- [ ] Add statistical significance testing (confidence intervals)
- [ ] Integrate with actual SFT checkpoint loader
- [ ] CARLA closed-loop evaluation

---

## Pipeline PR #5 (Daily Cadence)

**Focus:** PPO Stub for RL Refinement After SFT

### Changes

- **Added:** `training/rl/ppo_rl_refine_stub.py` - PPO stub implementing residual delta-waypoint learning
  - **SFT initialization**: `SFTWaypointModelStub` for loading pretrained waypoint BC models
  - **Residual delta head**: Learns `Δ = (y - ŷ) / σ` normalized correction
  - **Architecture**: `final_waypoints = sft_waypoints + delta_head(z)`
  - **PPO training**: GAE, clipping, value loss, entropy bonus
  - **Toy environment**: `ToyWaypointEnv` integration for kinematics testing
  - **Artifacts**: `out/<run_id>/metrics.json` and `train_metrics.json`

### Architecture

```
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│   SFT Model     │     │  Delta Head     │     │   Value Head    │
│  (frozen)       │     │  (trainable)    │     │  (trainable)    │
└────────┬────────┘     └────────┬────────┘     └────────┬────────┘
         │                       │                       │
         └───────────────────────┼───────────────────────┘
                                 │
                    ┌────────────▼────────────┐
                    │   final_waypoints =     │
                    │   sft_waypoints + Δ     │
                    └─────────────────────────┘
```

### Key Components

```python
@dataclass
class PPORLConfig:
    """PPO configuration for residual delta-waypoint learning."""
    sft_model_path: Optional[str] = None
    horizon: int = 20
    delta_hidden_dim: int = 64
    delta_lr: float = 1e-4
    ppo_lr: float = 3e-4
    clip_epsilon: float = 0.2
    gamma: float = 0.99
    lam: float = 0.95

class SFTWaypointModelStub(nn.Module):
    """Stub for SFT waypoint prediction (loads pretrained weights)."""
    ...

class DeltaWaypointHead(nn.Module):
    """Residual delta-waypoint head."""
    def forward(self, z) -> Tuple[torch.Tensor, torch.Tensor]:
        delta = self.delta_net(z)        # Δ prediction
        log_sigma = self.uncertainty_net(z)  # uncertainty
        return delta, log_sigma

class WaypointPolicy(nn.Module):
    """Combined SFT + delta policy."""
    def forward(self, z):
        sft_waypoints = self.sft_model(z)
        delta, _ = self.delta_head(z)
        value = self.value_head(z)
        return sft_waypoints, delta, value
```

### Loss Function

```
Loss = NLL + MSE + KL regularization

Δ_norm = (y - ŷ) / σ    # normalized delta
```

### Usage

```bash
# Run PPO training stub
python -m training.rl.ppo_rl_refine_stub --episodes 100 --lr 3e-4

# Output artifacts
out/ppo-delta-<timestamp>/metrics.json
out/ppo-delta-<timestamp>/train_metrics.json
```

### Output Format

```json
{
  "run_id": "ppo-delta-20260218-193000-abc123",
  "total_episodes": 100,
  "avg_reward": 2.34,
  "std_reward": 0.45,
  "avg_ade": 3.21,
  "avg_fde": 7.85,
  "best_reward": 5.12
}
```

### Pipeline Integration

RL refinement stage after SFT:
```
Waymo episodes → SSL pretrain → waypoint BC (SFT) → RL refinement (PPO) → eval
```

### Next Steps

- [ ] Integrate with actual SFT checkpoint loader
- [ ] Add KL regularization between SFT and RL policies
- [ ] CARLA closed-loop evaluation
- [ ] Compare offline ADE/FDE with closed-loop metrics
