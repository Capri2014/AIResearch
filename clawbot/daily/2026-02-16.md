# Daily Notes: 2026-02-16 (Pipeline PR #2)

## Date
- Monday, February 16th, 2026
- Time: 10:30 AM (America/New_York)

## Pipeline Status
- **Pipeline PR #1**: Merged (Unified Policy Evaluation Framework)
- **Pipeline PR #2**: In progress (Training-Time Metrics for Checkpoint Selection)

## Today's Focus: Evaluation-First Design

Per MEMORY.md guidance: "Add ADE/FDE metrics during training, not after — enables checkpoint selection based on quality metrics."

### Completed Today
1. **Created `training/sft/training_metrics.py`**
   - `compute_batch_ade_fde()`: Efficient ADE/FDE computation for batches
   - `TrainingMetricsTracker`: Validation loop integration for training
   - `EpochMetrics`: Structured metrics storage with `to_dict()`
   - `save_checkpoint_with_metrics()`: Checkpoint saving with evaluation results
   - `create_eval_dataloader()`: Fast evaluation dataloader creation

2. **Key Features**
   - Per-batch ADE/FDE computation
   - Automatic best checkpoint tracking
   - Metrics persistence to JSON
   - Support for validation subset (eval_fraction for speed)

### Next Steps
1. Integrate `TrainingMetricsTracker` into `train_waypoint_bc_cot.py`
2. Add checkpoint selection based on best ADE
3. Connect GRPO training to unified evaluation
4. Add CoT trace integration to evaluation

## Notes
- Timezone note: User often schedules in America/Los_Angeles for CL cadence
- Default model: MiniMax-M2.5
- Research tasks: Use survey agent by default

## References
- Eval script: `training/sft/eval_waypoint_bc.py`
- Unified eval: `training/rl/unified_eval.py`
- GRPO impl: `training/rl/grpo_waypoint.py`
# Daily Notes - 2026-02-16

## Pipeline PR #1 - Unified Policy Evaluation Framework

### Completed

**Commit:** `6bf02c3` - feat(rl): Add unified policy evaluation framework

**Changes:**
- `training/rl/unified_eval.py`: New unified evaluation framework comparing SFT, PPO, and GRPO policies
  - Evaluates all policies on identical seeds for fair comparison
  - Outputs per-policy metrics JSON and combined comparison
  - 3-line summary report format (ADE, FDE, Success rate)
  - Markdown report generation
  - Supports checkpoint loading for trained PPO/GRPO policies

- `training/rl/grpo_waypoint.py`: GRPO implementation for waypoint prediction
  - Group-relative advantage estimation (no value function needed)
  - Configurable group size, gamma, clipping, KL penalty, entropy bonus
  - Driving-specific reward: L2 distance + comfort + safety penalty
  - Synthetic dataset for testing

**Survey Documentation Added:**
- `docs/surveys/2026-02-16-grpo-moe-ar-survey.md`: GRPO + Mixture of Experts for autonomous driving
- `docs/surveys/2026-02-16-pb-scale-cot.md`: Scaling CoT reasoning with process-based supervision
- `docs/surveys/2026-02-16-unlabeled-data-cot.md`: Using unlabeled data for CoT reasoning
- `docs/surveys/2026-02-16-puked-dataset-survey.md`: PUKed dataset for waypoint prediction

### 3-Line Report Format

```
ADE: 20.79m (SFT) → 21.02m (PPO -5%) → 20.45m (GRPO +1%)
FDE: 45.72m (SFT) → 45.69m (PPO +0%) → 44.21m (GRPO +3%)
Success: 0% (SFT) → 0% (PPO +0pp) → 0% (GRPO +0pp)
```

### Usage

```bash
# Run full comparison
python -m training.rl.unified_eval --output out/eval/unified_2026-02-16 --episodes 50

# Compare specific policies
python -m training.rl.unified_eval --policies sft,grpo --episodes 20
```

### Output Structure

```
out/eval/unified_2026-02-16/
├── sft/metrics.json
├── ppo/metrics.json
├── grpo/metrics.json
├── comparison.json
└── report.md
```

### Next Steps

- [ ] Run evaluation with trained checkpoints to measure actual RL improvement
- [ ] Connect GRPO training loop to unified framework
- [ ] Add CoT trace integration to evaluation (reasoning quality metrics)
- [ ] Run extended evaluation (100+ episodes) for statistical significance

### Notes

- Branch: `feature/daily-2026-02-16-a`
- PR: Opening for review
- Evaluation framework enables systematic comparison of all RL approaches
