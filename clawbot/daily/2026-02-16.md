# Daily Notes - 2026-02-16

## Pipeline PR #1 - Unified Policy Evaluation Framework

### Completed

**Commit:** `6bf02c3` - feat(rl): Add unified policy evaluation framework

**Changes:**
- `training/rl/unified_eval.py`: New unified evaluation framework comparing SFT, PPO, and GRPO policies
  - Evaluates all policies on identical seeds for fair comparison
  - Outputs per-policy metrics JSON and combined comparison
  - 3-line summary report format (ADE, FDE, Success rate)
  - Markdown report generation
  - Supports checkpoint loading for trained PPO/GRPO policies

- `training/rl/grpo_waypoint.py`: GRPO implementation for waypoint prediction
  - Group-relative advantage estimation (no value function needed)
  - Configurable group size, gamma, clipping, KL penalty, entropy bonus
  - Driving-specific reward: L2 distance + comfort + safety penalty
  - Synthetic dataset for testing

**Survey Documentation Added:**
- `docs/surveys/2026-02-16-grpo-moe-ar-survey.md`: GRPO + Mixture of Experts for autonomous driving
- `docs/surveys/2026-02-16-pb-scale-cot.md`: Scaling CoT reasoning with process-based supervision
- `docs/surveys/2026-02-16-unlabeled-data-cot.md`: Using unlabeled data for CoT reasoning
- `docs/surveys/2026-02-16-puked-dataset-survey.md`: PUKed dataset for waypoint prediction

### 3-Line Report Format

```
ADE: 20.79m (SFT) → 21.02m (PPO -5%) → 20.45m (GRPO +1%)
FDE: 45.72m (SFT) → 45.69m (PPO +0%) → 44.21m (GRPO +3%)
Success: 0% (SFT) → 0% (PPO +0pp) → 0% (GRPO +0pp)
```

### Usage

```bash
# Run full comparison
python -m training.rl.unified_eval --output out/eval/unified_2026-02-16 --episodes 50

# Compare specific policies
python -m training.rl.unified_eval --policies sft,grpo --episodes 20
```

### Output Structure

```
out/eval/unified_2026-02-16/
├── sft/metrics.json
├── ppo/metrics.json
├── grpo/metrics.json
├── comparison.json
└── report.md
```

### Next Steps

- [ ] Run evaluation with trained checkpoints to measure actual RL improvement
- [ ] Connect GRPO training loop to unified framework
- [ ] Add CoT trace integration to evaluation (reasoning quality metrics)
- [ ] Run extended evaluation (100+ episodes) for statistical significance

### Notes

- Branch: `feature/daily-2026-02-16-a`
- PR: Opening for review
- Evaluation framework enables systematic comparison of all RL approaches
