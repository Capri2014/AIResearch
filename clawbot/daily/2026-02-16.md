# Daily Notes: 2026-02-16

**Pipeline PR #5** (4:30pm PT)

## Progress

### Feature Branch
- Created: `feature/daily-2026-02-16-e`

### New Files Added
- `training/rl/train_rl_delta_waypoint.py` - RL refinement after SFT with residual delta-waypoint learning
- `training/rl/run_rl_delta_smoke.py` - Smoke test for RL training pipeline

### Key Changes

1. **RL Refinement After SFT** (per MEMORY.md architecture)
   - Implements Option B: action space = waypoint deltas
   - Pattern: `final_waypoints = sft_waypoints + delta_head(z)`
   - Keeps SFT model frozen, only trains small delta head

2. **PPO Training Components**
   - `DeltaHead`: Small neural network predicting waypoint corrections (dx, dy)
   - `ValueHead`: Value function for PPO advantage estimation
   - `SFTWaypointModel`: Mock SFT model for demo (production: load checkpoint)
   - GAE (Generalized Advantage Estimation) for stable learning

3. **Training Pipeline**
   - Collect rollouts on `ToyWaypointEnv`
   - PPO update with clipped surrogate objective
   - Metrics: reward, length, KL divergence, delta norm
   - Checkpoint saving every N episodes
   - Output artifacts: `config.json`, `metrics.json`, `train_metrics.json`

4. **Smoke Test Results**
   ```
   Training: 10 episodes completed
   ✓ config.json created
   ✓ metrics.json created (2 entries)
   ✓ train_metrics.json created (10 episodes)
   ✓ final.pt checkpoint created
   ✓ checkpoints directory created (1 checkpoints)
   ```

## Next Steps
- Integrate with actual SFT checkpoint from `out/sft_waypoint_bc/model.pt`
- Connect to CARLA ScenarioRunner evaluation
- Add ADE/FDE metrics to RL training loop

---

**Pipeline PR #6** (6:30pm PT)

### Progress

#### Modified Files
- `training/rl/compare_sft_vs_rl.py` - Added git info tracking

#### Key Changes

1. **Git Info for Reproducibility**
   - Added `_git_info()` function capturing: repo URL, commit hash, branch
   - Included in metrics.json output for full reproducibility

2. **Deterministic Evaluation Run**
   - Ran 20 episodes for both SFT and RL policies on seeds 42-61
   - Generated `out/eval/<run_id>_sft/metrics.json` and `_rl/metrics.json`

3. **Results**
   ```
   ADE: 13.31m (SFT) → 13.03m (RL) [+2%]
   FDE: 37.17m (SFT) → 36.60m (RL) [+2%]
   Success: 0% (SFT) → 0% (RL) [+0%]
   ```

### Notes
- Both policies show 0% success due to max_steps=50 limit (toy env)
- Next: Integrate actual trained RL checkpoint from `train_rl_delta_waypoint.py`
- Future: Connect to CARLA ScenarioRunner for realistic evaluation
