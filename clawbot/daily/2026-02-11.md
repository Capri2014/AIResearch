# 2026-02-11 (PT) — ClawBot daily log

## What changed
- Added an **extended survey** of robotics + autonomous driving **pretraining tech**:
  - `docs/pretrain_tech_survey.md`
  - seeded `docs/references.md` with anchor directions
- Added missing **source stubs** for training wiring (CPU-only, minimal deps):
  - `training/pretrain/run_pretrain_stub.py` (+ README)
  - `training/sft/train_bc.py` (toy behavior cloning)
  - `training/rl/env_interface.py` + `training/rl/train_ppo_stub.py`
- Refreshed `clawbot/STATUS.md` (timestamp).

## Why (pretrain tech — the short version)
A lot of “foundation model” progress in Physical AI is *not one thing* — it’s a stack:

1) **Encoder pretraining** (vision/video/BEV): learn strong representations from lots of unlabeled or weakly-labeled data.
2) **Multimodal pretraining** (VLM): align image/video with text to get grounding.
3) **Behavior pretraining** (BC at scale): train policies on large logged action datasets to get a useful prior.
4) **World-model pretraining**: learn predictive dynamics for planning or auxiliary losses.

Robotics tends to emphasize (2)+(3) on multi-task episode datasets (RT-1/RT-2, Open X-Embodiment/RT-X, Octo) plus diffusion policies.
Driving tends to emphasize (1) BEV-centric representations and (3) imitation for planning heads (BEVFormer/BEVFusion/UniAD/VAD-like directions), with a growing world-model track (e.g., GAIA-1 direction).

## What to review in this PR
- `docs/pretrain_tech_survey.md`
- `docs/references.md`
- `training/**` stubs
- `clawbot/STATUS.md`

## Proposed next move (to make this concrete)
Pick **one** “first real experiment” path:
- **Driving-first:** CARLA closed-loop eval + BEV encoder + simple imitation head
- **Robotics-first:** MuJoCo/Isaac task suite + Octo-style fine-tune baseline

Once you pick, we can tighten the reading list and build the minimal runnable pipeline around it.
