# VADv3 Deep Dive

**Date:** 2026-02-17  
**Paper:** VADv3 (Vision-Action-Distilled v3)  
**Institution:** Horizon Robotics  
**Year:** 2025  
**Focus:** Multi-modal end-to-end autonomous driving

---

## TL;DR

VADv3 is Horizon Robotics' latest end-to-end autonomous driving model (2025). It extends VAD/VADv2 with **multi-modal capabilities** (camera + LiDAR + radar) and **distilled knowledge** from larger teacher models.

**Key Innovation:** Multi-modal fusion + knowledge distillation for efficient deployment on Journey chips.

---

## VAD Family Evolution

```
VAD (2022) ──→ VADv2 (2024) ──→ VADv3 (2025)
   │               │               │
   └─ Vectorized   └─ Improved    └─ Multi-modal
      planning        planning       + Distillation
```

| Version | Year | Key Contribution |
|---------|------|------------------|
| VAD | 2022 | Vectorized planning, end-to-end |
| VADv2 | 2024 | Improved perception, better planning |
| VADv3 | 2025 | Multi-modal + Knowledge distillation |

---

## Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                        VADv3 Architecture                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Multi-Modal Input:                                             │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐                        │
│  │  Camera │  │  LiDAR  │  │  Radar  │                        │
│  └────┬────┘  └────┬────┘  └────┬────┘                        │
│       │             │             │                             │
│       └─────────────┼─────────────┘                             │
│                     ▼                                           │
│              ┌─────────────┐                                   │
│              │   Encoder    │                                   │
│              │  (Shared)    │                                   │
│              └──────┬──────┘                                   │
│                     │                                           │
│        ┌────────────┼────────────┐                            │
│        ▼            ▼            ▼                            │
│  ┌──────────┐ ┌──────────┐ ┌──────────┐                     │
│  │ Perception│ │ Planning │ │ Control  │                     │
│  │   Head   │ │   Head   │ │   Head   │                     │
│  └──────────┘ └──────────┘ └──────────┘                     │
│                                                                  │
│         │              │              │                         │
│         ▼              ▼              ▼                         │
│    3D Boxes      Trajectory      Control                        │
│    Detection     Planning        Commands                       │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## Key Components

### 1. Multi-Modal Encoder

**Input:** Camera images + LiDAR point clouds + Radar data

**Fusion Strategy:**
- Early fusion at feature level
- Cross-modal attention
- Adaptive weighting per sensor

### 2. Perception Head

**Output:** 3D bounding boxes, lane detection, drivable area

### 3. Planning Head

**Output:** Vectorized trajectory (not rasterized)

**Key Idea:** Output directly executable by control modules.

### 4. Knowledge Distillation

**Teacher:** Larger driving model (possibly VAD-Teacher)

**Student:** VADv3 (efficient, deployable)

**What gets distilled:**
- Perception features
- Planning knowledge
- Safety margins

---

## Key Innovations

### 1. Multi-Modal Fusion

| Sensor | Strength | Weakness |
|--------|----------|----------|
| Camera | Color, texture, lane markings | Depth ambiguity, lighting |
| LiDAR | Accurate depth | Expensive, weather sensitive |
| Radar | Velocity, all-weather | Low resolution |

**VADv3 fuses all three** for robust perception.

### 2. Knowledge Distillation

```
Teacher Model (Large) ──────┐
                            │
                            ▼
                       Knowledge
                       Transfer
                            │
                            ▼
Student Model (VADv3) ─────┘
```

Benefits:
- Smaller model, similar performance
- Faster inference on Journey chips
- Better generalization

### 3. Vectorized Planning

**Not raster:** Output is trajectory coordinates, not a BEV image.

**Advantages:**
- Lower resolution requirement
- Direct control commands
- Interpretable

---

## Comparison with Other Approaches

| Aspect | VADv3 | UniAD | Tesla FSD |
|--------|-------|-------|-----------|
| Modality | Multi (cam+lidar+radar) | Multi | Camera only |
| Output | Vectorized | Vectorized | Neural net |
| Planning | End-to-end | End-to-end | E2E + modular |
| Distillation | Yes | No | No |
| Deployment | Journey chip | General GPU | Custom |

---

## How to Apply to Our Pipeline

### 1. Multi-Modal Input

We can add LiDAR/Radar data to our SSL encoder.

```
Current:  Images → SSL Encoder
Proposed: Images + LiDAR → Multi-Modal Encoder
```

### 2. Knowledge Distillation

Apply to our VLA + World Model pipeline:

```
Teacher: Full VLA + World Model
Student: Efficient deployment model
```

### 3. Vectorized Output

Keep our trajectory prediction as vectorized (already doing this).

---

## Code Structure (If Available)

```
training/
├── models/
│   └── vadv3/
│       ├── encoder.py      # Multi-modal encoder
│       ├── fusion.py       # Cross-modal attention
│       ├── perception.py   # Detection head
│       ├── planning.py     # Trajectory head
│       └── distillation.py # Knowledge transfer
```

---

## References

- VAD (ECCV 2022): Vectorized Autonomous Driving
- VADv2 (2024): Improved VAD
- VADv3 (2025): Multi-modal VAD with distillation
- Horizon Robotics Journey chips

---

## Questions for Implementation

1. **Data:** Do we have multi-modal data (camera + LiDAR)?
2. **Teacher:** Do we have a larger model to distill from?
3. **Deployment:** Target platform for VADv3?
4. **Benchmark:** How to evaluate VADv3 vs baseline?

---

## Related Papers in Our Repo

- `docs/papers/series-architecture-papers.md` - UniAD, GAIA-1 comparison
- `docs/surveys/2026-02-16-horizon-robotics.md` - Full Horizon survey
- `docs/surveys/2026-02-16-vla-world-model-2025-survey.md` - VLA surveys
